
# OriBuyin AI Agent

Lightweight FastAPI service that provides an AI-assisted chatbot and data-driven answers for the OriBuyin storefront. The service answers common product and ordering questions, returns top-selling products, high-rated items and category stats by querying the application's database, and falls back to OpenAI-powered responses when appropriate.

## Features
- Endpoints for asking questions to the AI Agent (FastAPI)
- Database-backed queries for best sellers, high-rating products, and category statistics
- Small OpenAI adapter (uses the `openai` Python package) to generate natural-language responses when needed
- Graceful handling of OpenAI errors (rate limits / insufficient quota) — returns structured error payloads instead of crashing
- Dockerfile and docker-compose for easy deployment

## Requirements
- Python 3.11
- MySQL-compatible database with the expected schema (products, order_items, reviews, categories)
- Environment variables configured (see below)

Dependencies are listed in `requirements.txt`.

## Environment variables
Create a `.env` file at the project root or supply environment variables in your deployment system. The service reads these variables (via python-dotenv):

- OPENAI_API_KEY - (required for AI responses) your OpenAI API key
- OPENAI_MODEL - (optional) model to use, default: `gpt-3.5-turbo`
- DB_HOST - MySQL host
- DB_USER - MySQL user
- DB_PASSWORD - MySQL password
- DB_NAME - MySQL database name

Example `.env`:

```
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-3.5-turbo
DB_HOST=host.docker.internal
DB_USER=root
DB_PASSWORD=secret
DB_NAME=oribuyin
```

## Run locally

1. Create a virtual environment and install deps:

```powershell
python -m venv .venv; .\.venv\Scripts\Activate.ps1; pip install -r requirements.txt
```

2. Set up your `.env` (see above).

3. Start the app with uvicorn:

```powershell
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

The service will be available at http://127.0.0.1:8000.

## Run with Docker

Build and run using Docker Compose (the compose file is provided):

```powershell
docker compose build
docker compose up
```

Notes:
- The Docker image installs the dependencies from `requirements.txt`. Ensure your `.env` values are present or set the environment in `docker-compose.yml`.
- If running MySQL on the host, `host.docker.internal` is added in `docker-compose.yml` to allow the container to reach the host's DB.

## API
All AI endpoints are under the `/ai` router.

- POST /ai/ask
	- Request JSON: { "question": "your question" }
	- Response JSON: { "answer": <payload> }

The `answer` payload is a dict with a `type` field and `data` (or an error message). Example payload types:

- `best_sellers` — `data` is a list of product rows (name, image, sold)
- `high_rating` — `data` is a list of product rows (name, image, avg_rating)
- `categories` — `data` is a list of category rows (name, image, purchases)
- `order_process`, `payment_info`, `shipping_info`, `return_policy` — canned text responses
- `ai_general` — fallback text response generated by OpenAI
- `ai_error` — structured error returned when LLM calls fail. Example:

```
{"type": "ai_error", "data": "rate_limit", "message": "You exceeded your current quota..."}
```

## Troubleshooting
- If the container fails with ModuleNotFoundError for `langchain_openai`, note that we intentionally use the `openai` package directly. Ensure `openai` is installed (listed in requirements).
- For OpenAI rate-limit or quota errors you'll receive an `ai_error` response. Check your billing plan and API usage.
- Database connection issues: verify the DB_* env vars and that the MySQL server is reachable from the container.

## Next improvements
- Convert the `/ai/ask` request body to a Pydantic model for stronger validation
- Map `ai_error` responses to proper HTTP error statuses (429/503) in the router
- Add retry/backoff for transient OpenAI errors
- Add unit/integration tests for the router and OpenAI adapter

## License
This project uses OriBuyin internal code — add license details here as appropriate.

